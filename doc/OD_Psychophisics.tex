% This is file `elsarticle-template-1-num.tex',
%
% Copyright 2009 Elsevier Ltd
%
% This file is part of the 'Elsarticle Bundle'.
% ---------------------------------------------
%
% It may be distributed under the conditions of the LaTeX Project Public
% License, either version 1.2 of this license or (at your option) any
% later version.  The latest version of this license is in
%    http://www.latex-project.org/lppl.txt
% and version 1.2 or later is part of all distributions of LaTeX
% version 1999/12/01 or later.
%
% Template article for Elsevier's document class `elsarticle'
% with numbered style bibliographic references
%
% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
\documentclass[preprint,review,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
% for a journal layout:
% \documentclass[final,1p,times]{elsarticle}
% \documentclass[final,1p,times,twocolumn]{elsarticle}
% \documentclass[final,3p,times]{elsarticle}
% \documentclass[final,3p,times,twocolumn]{elsarticle}
% \documentclass[final,5p,times]{elsarticle}
% \documentclass[final,5p,times,twocolumn]{elsarticle}


%%Use this packageto allow for accents 
\usepackage[utf8x]{inputenc}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%   round  -  round parentheses are used (default)
%   square -  square brackets are used   [option]
%   curly  -  curly braces are used      {option}
%   angle  -  angle brackets are used    <option>
%   semicolon  -  multiple citations separated by semi-colon
%   colon  - same as semicolon, an earlier confusion
%   comma  -  separated by comma
%   numbers-  selects numerical citations
%   super  -  numerical citations as superscripts
%   sort   -  sorts multiple citations according to order in ref. list
%   sort&compress   -  like sort, but also compresses numerical citations
%   compress - compresses without sorting
%
% \biboptions{comma,round}

\journal{Journal of Mathematical Psychology}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Bayesian Optimal Experimental Design for Generalized Linear Models.}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Manuel Villarreal, Carlos Vel\'{a}zquez and Arturo Bouzas}

\address{Mexico City, Mexico}

\begin{abstract}
%% Text of abstract
\end{abstract}

\begin{keyword}
Optimal experimental design \sep Generalized linear models \sep Psycophysics \sep Change point detection
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers

%% main text
\section{Introduction}
\label{S:1}

One of the main challenges in scientific research is the design of an experiment. A good experimental design will make the difference between finding an answer to our research question and wasting valuable resources. Optimal Experimental Design (OED) allows us to re-interpret the design of an experiment as a decision problem where the objective is to maximize a utility function. This function is a numeric representation of our preferences over the possible results of the experiment.

To optimize a design, first we need to identify which of our variables are subject to this procedure. For example, we might want to select which values of an independent variable should be tested, or how many times we should test each of those values. Most of the time, these choices are made on the basis of previous research. However, it might be the case that there is not enough information to make these decisions with confidence, or that the values that are commonly used, do not allow for strong conclusions. Second, we need to formalize the objective of the experiment through a utility function. For example, if our objective is to discriminate between the predictions of two cognitive models, we can select a function wich assigns a greater value to designs were their predictions are further appart. There are two examples in the Psychological literature that implement different utility functions \citep[e.g.][]{Myung2009,ZL2010}. In both, the objective of the design is to discriminate between competing cognitive models, however, considerations of what makes a model ``good'' drive the authors to choose different functions.

% 1. ADD CITATIONS
In this paper, we will present a different approach where the problem is not to select between models but to make inferences about the parameters of a single one. In particular, we will present an example of OED with a model that is commonly used in Psychophysics and Decision Making, the logistic response model. %1
In this example, we will optimize an experimental design focused on studying the ability of participants to detect changes in the success rate of a probabilistic series. Additionally, we will show how different prior assumptions can change the results obtained from this process, this will highlight the importance of the choice of prior distributions in the process of designing an experiment. It is important to note that the aproach presented here follows a Bayesian perspective. A good introduction to Bayesian and other approaches to OED for Generalized Linear Models (GLM), and their limitations, can be found in \citet{khurietal2006}.

The rest of the paper will be organized as follows. First, we will introduce the concepts of OED particularly in the field of GLM's. Second, we will present a brief summary of the experimental problem being addressed and, a computational model which has been used to account for the behavior of human participants under similar settings. Finally, we will present the results of the optimization procedure under different prior distributions, including one generated from the model responses to simulations of the experiment.

\section{Optimal experimental design}

The concepts of OED from a decision theoretic perspective were introduced by \citet{Lindley1972}. In this work, the author argued that the design of an experiment should be approached as a decision problem. In general, one needs to specify a utility function reflecting the purpose of the experiment and then select the design which maximizes expected utility. It is also worth noting that in practice, this process will rely on the assumption of an underlying data generating model. For example, in their paper, \citet{Myung2009} use the tools of OED to optimize an experiment to discriminate between two memory retention models. In this case, the design found will be optimal only in the case that either model is true.

Under the OED framework, the problem of designing an experiment reduces to selecting a design $\eta$ from a set $H$, where data $\mathbf{y}$ will be observed given the design and a model's parameters $\theta$. The utility function $U(\eta,\mathbf{y},\theta)$, maps the values of these variables to the Real numbers. For this example, the objective will be to select the $n$ values of an independent variable $x$ from an experimental space $X$. This space can be thought of as the numerical variables which can be controlled by the experimenter. The design $\eta$, will be the weight of our observations assigned to each of the $n$ design points $\eta_i=n_i/n$, with the constraint that  $\sum_{i=1}^{n}\eta_i = 1$. In this case, $\eta$ can be interpreted as a probability measure over the experimenral space and $H$ as the set of all such measures.

The expected utility of a design $\eta$ is represented by the following equation:
\begin{equation}
U(\eta)=\int \int U(\eta,\mathbf{y},\theta)p(\theta|\mathbf{y},\eta)p(\mathbf{y}|\eta) d\theta d\mathbf{y}
\label{eu}
\end{equation}
where $p(\theta|\mathbf{y},\eta)$ represents the posterior probability of $\theta$ given the design $\eta$ and data $\mathbf{y}$ and, $p(\mathbf{y}|\eta)$ is the predictive distribution of $\mathbf{y}$. Finding an optimal design, reduces to the problem of finding the values for the variables in $\eta$ for which equation \ref{eu} takes it's maximum value. For many applications the requiered integration is not a trivial problem, therefore, one could rely on analytic or numeric approximations to solve it.

Many utility functions have been proposed for both linear and non-linear design problems, however, as was previously mentioned, the choice of a utility function depends on the research question. For example, \cite{Myung2009} used the sum of squered errors between the data generated by a model and the predictions of a competing one given an expeirmental design. This is because, the objective is to find a design that can discriminate the predictions of the two. On the other hand, \cite{ZL2010} choose a utility function based on the Bayes Factor. Both functions will have their advantages, nontheless, the first, emphasizes disciminability of model's predictions, while the second aims for a design in which the data generating model is more likely.

When the objective of the experiment is to make an inference about the parameters of a generalized linear model, some authors \citep[e.g.][]{Ber1979} have proposed to consider the gain in Shanon's Information as a utility function. In this case, the objective is to find the design $\eta$ that maximizes the expected gain in Shanon Information, or equvalently, maximizes the Kullback-Leibler divergence between the posterior and prior distribution. With this function, the expected utility of an experimental design is defined as:
\begin{equation}
U(\eta)=\int \int log\frac{p(\theta|y,\eta)}{p(\theta)} p(y,\theta |\eta) d\theta dy
\label{klu}
\end{equation}
The prior distribution in the denominator of the logarithm can be droped as it does not depend on the experimental design, therefore, the optimal design will be the one that maximizes:
\begin{equation}
U(\eta)=\int \int log \{p(\theta|y,\eta)\} p(y,\theta |\eta) d\theta dy
\label{egShanon}
\end{equation}
which is the posterior expected Shanon information. The objective of this function is to minimize the posterior variance of the model's parameters, however, there are other functions that can be used for design optimization for GLM's. A good overview of some of this functions for linear and non-linear design problems can be found in \citet{ChalonerVerdinelli1995}. 

One of the problems when dealing with design optimization for GLM's is that the posterior distribution of the parameter vector $\theta$ is not always tractable. However, it is common to use the normal approximation to the posterior distribution
\begin{equation}
\theta|y,\eta \sim N\left(\hat{\theta},[nI(\hat{\theta},\eta)]^{-1}\right)
\label{Naprox}
\end{equation}
Where $I(\hat{\theta},\eta)$ denotes the observed fisher information matrix given an experimental design and $\hat{\theta}$ is the maximum likelihood estimate of $\theta$. Even with this, the marginal distribution of the data $p(\mathbf{y}|\eta)$ in equation \ref{eu} needs to be aproximated. In this case, according to \cite{chalar1989}, we can take the prior distribution of $\theta$ as the predictive distribution of $\hat{\theta}$ in order to approximate the posterior expected utility.

With both aproximations, the value of $U(\eta)$ becomes:
\begin{equation}
U(\eta)=-\frac{k}{2}log(2\pi)-\frac{k}{2}+\frac{1}{2} \int log \{det[nI(\theta,\eta)]\} p(\theta) d\theta
\label{aproxu1}
\end{equation}
Equation \ref{aproxu1} gives the exact expected utility of an experimental design. However, one could drop the constant and multiplier terms giving the following form:
\begin{align}
\phi(\eta) & = \int log \{det[nI(\theta,\eta)]\} p(\theta) d\theta \\
           & = \mathbb{E}_\theta \left( log\{det[nI(\theta,\eta)]\} \right)
\label{crit1}
\end{align}
the function $\phi(\eta)$ is known as a design criterion. An optimal design would be the one maximizing equation \ref{crit1}. It is worth noting that this relies on the normal approximation to the posterior distribution in \ref{Naprox}, therefore, for small samples there are some constraints that can help to assure normality \citep[see][]{CLCH2002}. This kind of criteria can be found for other utility functions. For example, \citet{chalar1989} presented the derivation and application of two design criteria, one of which was $\phi(\eta)$. The second one, aimed to optimize a design where certain predictions of the model are of interest. 

Finally, to test wether a particular design $\eta_0$ is optimal, one can use the concept of a directional derivative. The definition and conditions for this test can be found in more detail in \citet{chalar1989}. As the authours suggest, we denote $\eta_x$ as a design which puts all the weight on a single point $x$ in the experimental space $X$ and $d(\eta,x)$ as the derivative of $\eta$ in the direction of $\eta_x$. A design $\eta_0$ is optimal if the roots of the directional derivative are in the support of $\eta_0$ and it is lower for any other point in $X$. For the selected criterion $\phi$, the directional derivative is equal to:
\begin{equation}
d(\eta,x)=\mathbb{E}_\theta\ tr \left( I(\theta,\eta_x) I(\theta,\eta)^{-1}\right)-p
\end{equation}
where $p$ represents the number of parameters in the logistic model and $tr(\cdot)$ is the represents the trace.



\section{Perception of change in probabilistic series}
\label{S:2}



\section{Results}
\label{S:3}

\subsection{Consruction of the prior distribution}


\subsection{Optimal design}

\begin{figure}
\includegraphics[width=\textwidth]{Prior_and_Predictive.pdf}
\caption{Prior and predictive prior distributions.}
\label{fig:ppd}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{Support_detadx.pdf}
\caption{Optimal experimental designs and directional derivatives.}
\label{fig:detadx}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{Joint_Normal.pdf}
\caption{Informative prior distribution, prior predictive, optimal design and directional derivative.}
\label{fig:jn}
\end{figure}

\section{Discussion}
\label{S:4}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

\section*{References}
\bibliographystyle{elsarticle-harv}
\biboptions{authoryear}
\bibliography{OD_Psychophisics}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRO PRARAGRAPHS REMOVED %%%%%%%%%%%%%%%%%%%%%%%%%%%%

% When designing an experiment, often one starts by making decisions about the number of participants, how many and what values of our independent variable we should test or how many times should we test those values. Most of the time, these choices are made on the basis of previous research on the field. However, it might be the case that there is not enough information to make these decisions with confidence, or that the values that are commonly used, do not allow for strong conclusions. Optimal Experimental Design (OED) offers an alternative to solve this kind of problems through a re-interpretation of the experimental design as a desicion problem in which the objective is to maximaze a utility function.

% This function is a numeric representation of our preference over the possible consequences of running the experiment. Therefore, the optimization of an experimental design requires us to have a formal interpretation of the purpose of the experiment.
%For example, \citet{Myung2009} use a function that assigns a greater utility to designs that can discriminate between the predictions of two competing models, whereas, \citet{ZL2010} assign greater value to designs where the data generating model is the most likely. The objective in both examples is the same, however, the considerations of what makes a model good, play a role in the selection of the function. In the first case, models should predict different data patterns, in the second, the model that is favored by the evidence should be the one that generated the data.

%The objective of this paper is to present an introductory example of OED with a model that is commonly used in the Psychophysical and Decision Making literature, the logistic response model. 

%The concept of optimizing an experimental design is not new to psychological research. There are already examples of design optimization in the literature \citep[e.g.][]{Myung2009,ZL2010}. Both of this examples discuss and demonstrate the advantages of OED for model comparison in psychology. 

%In particular, we will present an example of OED in the context of Generalized Linear Models. These models are widely used in psychophysics. The problem that will be treated here is fairly new, however, the methodological aspects remain the same even with more straight-forward psychophysical experiments. In order to do this, we will use a particular parametrization of the logistic model which is primarily used in statistics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OED PRARAGRAPHS REMOVED %%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Once we have defined the design space $\eta$, the objective of the experiment and the prior informartion $p(\theta)$, the expected utility of a design is represented by the following equation:
% \begin{equation}
% U(\eta)=\int \int U(\eta,y,\theta)p(\theta|y,\eta)p(y|\eta) d\theta dy
% \label{eu}
% \end{equation}
% Therefore, finding the experimental design that is optimal given a utility function, reduces to the problem of finding the values for the variables in $\eta$ for which equation \ref{eu} takes it's maximum value.

%To apply the concepts of OED to a particular problem, first, we need to define the elements of the design space. This space is conformed by the variables that we can manipulate during the experiment, for example, the values that our independent variable might take or the weight (proportion of observations) assigned to each of those values. These elements are the ones that we can modify in order to optimize the design.

%The second step would be to formalise the objective of the experiment. For example, in the case of a logistic model, we might want to find the values of our independent variable that minimize the variance of the model parameters, or we might be interested in the magnitude of the physical stimulus for which the probability of a response takes on a certain value. The formalization of the research question will define a utility function.

%The last step is to specify the prior information that we have about the problem at hand. This last step can be carried out in two ways, first, we can try to optimize the experiment for a particular guess about the parameter values of the model of interest, or we could use a probability distribution to account for the uncertainty in the values that we are interested in. This last step is primarily important to the optimization process in generalized linear models, because the optimal design will depend on the values of the parameters.

% Nevertheless,  when the posterior utility only depends on $y$ through some constistent estimate of $\hat{\theta}$, a further approximation is to take the predictive distribution of  $\hat{\theta}$ to be the prior distribution \cite{chalar1989}.


%%
%% End of file `elsarticle-template-1-num.tex'.